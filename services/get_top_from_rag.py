import asyncio
import json
import os
import numpy as np
import base64
import io
from typing import List, Dict, Any, Tuple
from openai import AsyncOpenAI

# =======================
# å·¥å…·å‡½æ•°ï¼šè°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ Embedding APIï¼ˆå¼‚æ­¥ï¼‰
# =======================
async def bailian_embed_async(text: str, model: str = "text-embedding-v4") -> List[float]:
    """
    å¼‚æ­¥è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ Embedding API (text-embedding-v4)
    ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
    """
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise ValueError("ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®")

    # åˆ›å»º AsyncOpenAI å®¢æˆ·ç«¯
    client = AsyncOpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1" # æ³¨æ„ï¼šç§»é™¤äº†æœ«å°¾ç©ºæ ¼
    )

    target_dim = 1024
    try:
        # è°ƒç”¨ Embeddings API
        response = await client.embeddings.create(
            model=model,
            input=text,
            dimensions=target_dim,
            encoding_format="float"
        )
        # æå–åµŒå…¥å‘é‡
        embedding = response.data[0].embedding
        return embedding
    except Exception as e:
        print(f"âŒ è°ƒç”¨ Embedding API æ—¶å‡ºé”™: {e}")
        # æ ¹æ®éœ€æ±‚è¿”å›é›¶å‘é‡æˆ–é‡æ–°æŠ›å‡ºå¼‚å¸¸
        # è¿™é‡Œé€‰æ‹©è¿”å›é›¶å‘é‡ä»¥é¿å…ä¸­æ–­
        return [0.0] * target_dim

# =======================
# å·¥å…·å‡½æ•°ï¼šè§£ç  Base64 å­—ç¬¦ä¸²ä¸º NumPy æ•°ç»„
# =======================
# =======================
# å·¥å…·å‡½æ•°ï¼šè§£ç  Base64 å­—ç¬¦ä¸²ä¸º NumPy æ•°ç»„ (ä¿®æ­£ç‰ˆ - æ”¯æŒ pickle)
# =======================
# =======================
# å·¥å…·å‡½æ•°ï¼šè§£ç  Base64 å­—ç¬¦ä¸²ä¸º NumPy æ•°ç»„ (ä¿®æ­£ç‰ˆ - ç›´æ¥è§£é‡Šä¸º float32)
# =======================
def decode_base64_vector_matrix(base64_str: str, num_vectors: int, vector_dim: int = 1024, dtype: np.dtype = np.float32) -> np.ndarray:
    """
    å°† Base64 ç¼–ç çš„åŸå§‹ float32 å‘é‡æ•°æ®è§£ç ä¸º NumPy æ•°ç»„ã€‚
    
    å‡è®¾åŸå§‹æ•°æ®æ˜¯ num_vectors ä¸ª vector_dim ç»´çš„ dtype ç±»å‹å‘é‡çš„è¿ç»­äºŒè¿›åˆ¶æ•°æ®ã€‚
    """
    try:
        # 1. Base64 è§£ç 
        decoded_bytes = base64.b64decode(base64_str)
        print(f"âœ… è§£ç  matrix å­—ç¬¦ä¸²ä¸º {len(decoded_bytes)} å­—èŠ‚ã€‚")

        # 2. éªŒè¯å­—èŠ‚é•¿åº¦æ˜¯å¦åŒ¹é…é¢„æœŸ
        expected_bytes = num_vectors * vector_dim * dtype().itemsize # itemsize for float32 is 4
        if len(decoded_bytes) != expected_bytes:
            print(f"âš ï¸ è­¦å‘Š: è§£ç åçš„å­—èŠ‚æ•° ({len(decoded_bytes)}) ä¸é¢„æœŸ ({expected_bytes}) ä¸ç¬¦ã€‚")
            # å¯ä»¥é€‰æ‹©è¿”å›ç©ºæ•°ç»„æˆ–å°è¯•å¤„ç†
            # return np.array([]) # æˆ–è€…æŠ›å‡ºå¼‚å¸¸

        # 3. å°†å­—èŠ‚é‡æ–°è§£é‡Šä¸º NumPy æ•°ç»„
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ•°æ®æ˜¯ä»¥å°ç«¯åº (little-endian) å­˜å‚¨çš„
        array_flat = np.frombuffer(decoded_bytes, dtype=dtype)
        
        # 4. é‡å¡‘ä¸º (num_vectors, vector_dim) çš„çŸ©é˜µ
        array_matrix = array_flat.reshape((num_vectors, vector_dim))
        
        print(f"âœ… å°†å­—èŠ‚é‡å¡‘ä¸º NumPy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º {array_matrix.shape}ï¼Œæ•°æ®ç±»å‹ä¸º {array_matrix.dtype}ã€‚")
        return array_matrix

    except Exception as e:
        print(f"âŒ è§£ç /é‡å¡‘ matrix æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        # æ ¹æ®éœ€è¦å¤„ç†å¼‚å¸¸ï¼Œä¾‹å¦‚è¿”å› None æˆ–æŠ›å‡ºç‰¹å®šå¼‚å¸¸
        raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿è°ƒç”¨è€…å¤„ç†

# =======================
# ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
# =======================
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """è®¡ç®—ä¸¤ä¸ª NumPy å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    # ç¡®ä¿æ˜¯ 1D å‘é‡
    a = a.ravel()
    b = b.ravel()
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0 # é¿å…é™¤ä»¥é›¶
    return np.dot(a, b) / (norm_a * norm_b)

# =======================
# åŠ è½½ vdb_chunks.json æ•°æ®
# =======================
def load_vdb_chunks(file_path: str) -> Tuple[List[Dict[str, Any]], np.ndarray]:
    """
    ä»æŒ‡å®šè·¯å¾„åŠ è½½ vdb_chunks.json æ–‡ä»¶ï¼Œ
    å¹¶è§£ç  'matrix' å­—æ®µä¸­çš„ Base64 ç¼–ç çš„ NumPy æ•°ç»„ã€‚
    è¿”å› (data åˆ—è¡¨, è§£ç åçš„çŸ©é˜µ NumPy æ•°ç»„)ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks_data = data.get("data", [])
        raw_matrix_string = data.get("matrix", "") # æ³¨æ„ï¼šè¿™é‡Œæ˜¯å•ä¸ªå­—ç¬¦ä¸²
        
        print(f"âœ… ä» data åˆ—è¡¨åŠ è½½äº† {len(chunks_data)} ä¸ª chunksã€‚")

        if not raw_matrix_string:
             print("âš ï¸ è­¦å‘Š: JSON æ–‡ä»¶ä¸­ 'matrix' å­—æ®µä¸ºç©ºæˆ–ç¼ºå¤±ã€‚")
             return chunks_data, np.array([]) # è¿”å›ç©ºæ•°ç»„

        # è§£ç  Base64 ç¼–ç çš„ NumPy çŸ©é˜µ
        try:
            # ä½¿ç”¨å‡½æ•°è§£ç  matrix å­—ç¬¦ä¸² (ä»… Base64 è§£ç  + NumPy åŠ è½½)
            num_chunks = len(chunks_data)
            # ä½¿ç”¨ä¿®æ­£åçš„å‡½æ•°è§£ç  matrix å­—ç¬¦ä¸² (Base64 è§£ç  -> å­—èŠ‚ -> float32 æ•°ç»„ -> reshape)
            matrix_data_np: np.ndarray = decode_base64_vector_matrix(raw_matrix_string, num_vectors=num_chunks, vector_dim=1024, dtype=np.float32)
        except Exception as e:
             print(f"âŒ è§£ç  'matrix' å­—æ®µå¤±è´¥: {e}")
             return chunks_data, np.array([]) # è¿”å›ç©ºæ•°ç»„

        # éªŒè¯çŸ©é˜µå½¢çŠ¶
        if matrix_data_np.size > 0:
            expected_rows = len(chunks_data)
            actual_rows, actual_cols = matrix_data_np.shape
            if actual_rows != expected_rows:
                print(f"âš ï¸ è­¦å‘Š: çŸ©é˜µè¡Œæ•° ({actual_rows}) ä¸ data åˆ—è¡¨é•¿åº¦ ({expected_rows}) ä¸åŒ¹é…ã€‚")
                # å¯ä»¥é€‰æ‹©æˆªæ–­æˆ–å¡«å……ï¼Œè¿™é‡Œç®€å•è­¦å‘Š
            if actual_cols != 1024: # å‡è®¾ç»´åº¦æ˜¯ 1024ï¼Œæ ¹æ®ä½ çš„ embedding_dim è°ƒæ•´
                 print(f"âš ï¸ è­¦å‘Š: çŸ©é˜µåˆ—æ•° ({actual_cols}) ä¸æ˜¯ 1024ã€‚")
            else:
                 print(f"âœ… çŸ©é˜µå½¢çŠ¶ä¸€è‡´: {matrix_data_np.shape} å¯¹åº” {expected_rows} ä¸ª chunksã€‚")
        else:
             print("âš ï¸ è­¦å‘Š: è§£ç åçš„çŸ©é˜µä¸ºç©ºã€‚")

        return chunks_data, matrix_data_np

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        return [], np.array([])
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: æ— æ³•è§£æ JSON æ–‡ä»¶ {file_path}: {e}")
        return [], np.array([])
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return [], np.array([])

# =======================
# ä¸»æŸ¥è¯¢å’Œç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°
# =======================
async def query_and_find_topk(query_text: str, vdb_file_path: str = "./rag_storage/vdb_chunks.json", topk: int = 5):
    """
    æŸ¥è¯¢æ–‡æœ¬ï¼Œå¹¶ä» vdb_chunks.json ä¸­æ‰¾åˆ° topk ä¸ªæœ€ç›¸ä¼¼çš„æ¡ç›®
    ä½¿ç”¨é¢„å…ˆè®¡ç®—å¹¶è§£ç å¥½çš„ matrix å‘é‡
    """
    print(f"ğŸ” æ­£åœ¨ä»: {vdb_file_path} åŠ è½½æ•°æ®...")
    
    # 1. åŠ è½½æ•°æ®å’Œé¢„è®¡ç®—çš„å‘é‡ (å·²è§£ç )
    chunks_data, matrix_data_np = load_vdb_chunks(vdb_file_path)
    
    if not chunks_data or matrix_data_np.size == 0:
        print("âš ï¸ è­¦å‘Š: æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®æˆ–å‘é‡ã€‚")
        return

    num_chunks = len(chunks_data)
    num_vectors = matrix_data_np.shape[0] if matrix_data_np.size > 0 else 0
    print(f"âœ… æˆåŠŸåŠ è½½ {num_chunks} ä¸ªæ¡ç›®å’Œå¯¹åº”çš„å‘é‡çŸ©é˜µ ({num_vectors} è¡Œ)ã€‚")

    # 2. è·å–æŸ¥è¯¢æ–‡æœ¬çš„ embedding (ä½¿ç”¨ AsyncOpenAI API)
    print("ğŸ” æ­£åœ¨è·å–æŸ¥è¯¢æ–‡æœ¬çš„ Embedding (é€šè¿‡é˜¿é‡Œäº‘ç™¾ç‚¼ API)...")
    try:
        query_embedding_list = await bailian_embed_async(query_text)
        # è½¬æ¢ä¸º NumPy æ•°ç»„ä»¥ä¾¿è®¡ç®—ï¼Œç¡®ä¿å½¢çŠ¶ä¸º (1024,)
        query_embedding_np = np.array(query_embedding_list, dtype=np.float32).ravel() # .ravel() ç¡®ä¿æ˜¯ 1D
        print(f"âœ… æŸ¥è¯¢ Embedding å½¢çŠ¶: {query_embedding_np.shape}")
    except ValueError as e:
        print(f"âŒ Embedding é”™è¯¯: {e}")
        return
    except Exception as e:
        print(f"âŒ è·å–æŸ¥è¯¢ Embedding æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return

    # 3. è®¡ç®—æ‰€æœ‰æ¡ç›®çš„ç›¸ä¼¼åº¦ (ä½¿ç”¨é¢„è®¡ç®—å¹¶è§£ç çš„å‘é‡çŸ©é˜µ)
    print("ğŸ” æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦...")
    similarities_and_chunks: List[Tuple[float, Dict[str, Any]]] = []

    # ä½¿ç”¨çŸ©é˜µè¿ç®—è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦ä¼šæ›´é«˜æ•ˆï¼Œä½†å¾ªç¯ä¹Ÿé€‚ç”¨ä¸”æ›´ç›´è§‚
    # å‡è®¾ matrix_data_np å½¢çŠ¶æ˜¯ (N, 1024)
    for i in range(matrix_data_np.shape[0]):
        try:
            # å–å‡ºç¬¬ i ä¸ª chunk çš„ embedding å‘é‡
            content_embedding_np = matrix_data_np[i] # å½¢çŠ¶ (1024,)
            # è®¡ç®—ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
            sim = cosine_similarity(query_embedding_np, content_embedding_np)
            # å°†ç›¸ä¼¼åº¦å’Œå¯¹åº”çš„ chunk ä¿¡æ¯å­˜å…¥åˆ—è¡¨
            # æ³¨æ„ï¼šç¡®ä¿ç´¢å¼• i åœ¨ chunks_data èŒƒå›´å†…ï¼ˆè™½ç„¶åŠ è½½æ—¶å·²å¯¹é½ï¼‰
            if i < len(chunks_data):
                 similarities_and_chunks.append((sim, chunks_data[i]))
            else:
                 print(f"âŒ ç´¢å¼•ä¸åŒ¹é…: çŸ©é˜µè¡Œ {i} æ²¡æœ‰å¯¹åº”çš„æ•°æ®å—ã€‚")
        except Exception as e:
            print(f"âŒ å¤„ç†æ¡ç›®ç´¢å¼• {i} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue

    if not similarities_and_chunks:
        print("âš ï¸ è­¦å‘Š: æœªèƒ½è®¡ç®—ä»»ä½•ç›¸ä¼¼åº¦ã€‚")
        return

    # 4. æ’åºå¹¶è·å– Top-K
    print(f"ğŸ” æ­£åœ¨æ’åºå¹¶è·å– Top-{topk}...")
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
    similarities_and_chunks.sort(key=lambda x: x[0], reverse=True)
    topk_results = similarities_and_chunks[:topk]

    # 5. è¾“å‡ºç»“æœ
    print("\n--- ğŸ“‹ æŸ¥è¯¢ç»“æœ (Top-K) ---")
    print(f"ğŸ” æŸ¥è¯¢æ–‡æœ¬: {query_text}\n")
    # print(topk_results)
    for i, (similarity, chunk) in enumerate(topk_results):
        content = chunk.get("content", "N/A")
        file_path = chunk.get("file_path", "N/A")
        chunk_id = chunk.get("__id__", "N/A")
        created_at = chunk.get("__created_at__", "N/A")
        print(f"--- ğŸ† Top {i+1} (ç›¸ä¼¼åº¦: {similarity:.4f}) ---")
        print(f"ğŸ†” ID: {chunk_id}")
        print(f"ğŸ•’ åˆ›å»ºæ—¶é—´: {created_at}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
        # æ‰“å°å†…å®¹çš„å‰ N ä¸ªå­—ç¬¦
        print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {content[:500]}...\n")

    formatted_results = []
    for similarity, chunk in topk_results:
            # æ„é€ ä¸€ä¸ªæ–°çš„å­—å…¸ï¼ŒåŒ…å«åŸå§‹ chunk ä¿¡æ¯å’Œç›¸ä¼¼åº¦
            result_item = {
                "similarity": similarity,
                "content": chunk.get("content", ""),
                "file_path": chunk.get("file_path", ""),
                "chunk_id": chunk.get("__id__", ""),
                # å¯ä»¥æ·»åŠ å…¶ä»–éœ€è¦çš„å­—æ®µ
            }
            formatted_results.append(result_item)
    
    print("\n--- æŸ¥è¯¢ç»“æœ (Top-K) ---")
    print(f"æŸ¥è¯¢æ–‡æœ¬: {query_text}\n")
    for i, item in enumerate(formatted_results): # ä½¿ç”¨ formatted_results æ‰“å°
        # ä» item å­—å…¸ä¸­è·å–ä¿¡æ¯
        content = item.get("content", "N/A")
        file_path = item.get("file_path", "N/A")
        chunk_id = item.get("chunk_id", "N/A")
        similarity_score = item.get("similarity", 0.0) # è·å–ç›¸ä¼¼åº¦
        print(f"--- Top {i+1} (ç›¸ä¼¼åº¦: {similarity_score:.4f}) ---")
        print(f"ID: {chunk_id}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"å†…å®¹: {content[:500]}...\n")
        
    # è¿”å›æ ¼å¼åŒ–åçš„å­—å…¸åˆ—è¡¨
    return formatted_results
    

# =======================
# å…¥å£å‡½æ•°
# =======================
async def main():
    # --- é…ç½® ---
    query_text = "è¯­ä¹‰å›¾åƒï¼Ÿ"
    vdb_file_path = "./rag_storage/vdb_chunks.json" # è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
    topk = 5
    # --- é…ç½®ç»“æŸ ---

    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âŒ é”™è¯¯: ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ã€‚è¯·è®¾ç½®åé‡è¯•ã€‚")
        return # ä¼˜é›…é€€å‡º

    result = await query_and_find_topk(query_text, vdb_file_path, topk)
    return result

# =======================
# è¿è¡Œ
# =======================
if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªå¤„ç†çš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
